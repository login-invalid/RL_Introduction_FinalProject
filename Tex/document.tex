\documentclass{article}
\usepackage[pdftex,active,tightpage]{preview}
\setlength\PreviewBorder{2mm}

\usepackage[utf8]{inputenc} % this is needed for umlauts
\usepackage[ngerman]{babel} % this is needed for umlauts
\usepackage[T1]{fontenc}    % this is needed for correct output of umlauts in pdf
\usepackage{amssymb,amsmath,amsfonts} % nice math rendering
\usepackage{braket} % needed for \Set
\usepackage{caption}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\DeclareCaptionFormat{myformat}{#3}
\captionsetup[algorithm]{format=myformat}

\begin{document}
	
\begin{preview}
	\begin{algorithm}[H]
		\begin{algorithmic}
			\Require
			\Statex Sates $\mathcal{S} = \{1, \dots, n_s\}$
			\Statex Actions $\mathcal{A} = \{1, \dots, n_a\}$
			\Statex Reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$
			\Statex Black-box (probabilistic) transition function $T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$
			\Statex Learning rate $\alpha \in [0, 1]$, typically $\alpha = 0.1$
			\Statex Discounting factor $\gamma \in [0, 1]$
			\Statex $\lambda \in [0, 1]$: Trade-off between TD and MC
			\Procedure{SARSA-lambda}{$\mathcal{S}$, $A$, $R$, $T$, $\alpha$, $\gamma$, $\lambda$}
			\State Initialize $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ arbitrarily
			\State Initialize $e: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ with 0. \Comment{eligibility trace}
			\State Start in state $s_0 \in \mathcal{S}, s \gets s_0$
			\While{$Q$ is not converged}
			\State Initialize $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ arbitrarily except set terminal states zero.
			\While{$s$ is not terminal}
			\State $r \gets R(s, a)$
			\State $s' \gets T(s, a)$ \Comment{Receive the new state}
			\State Calculate $\pi$ based on $Q$ (e.g. epsilon-greedy)
			\State $a' \gets \pi(s')$
			\State $e(s, a) \gets e(s, a) + 1$
			\State $\delta \gets r + \gamma \cdot Q(s', a') - Q(s, a)$
			\Comment{Temporal Difference}
			\For{$(\tilde{s}, \tilde{a}) \in \mathcal{S} \times \mathcal{A}$}
			\State $Q(\tilde{s}, \tilde{a}) \gets Q(\tilde{s}, \tilde{a}) + \alpha \cdot \delta \cdot e(\tilde{s}, \tilde{a})$
			\State $e(\tilde{s}, \tilde{a}) \gets \gamma \cdot \lambda \cdot e(\tilde{s}, \tilde{a})$
			\Comment{Update eligibility trace}
			\EndFor
			\State $s \gets s'$
			\State $a \gets a'$
			\EndWhile
			\EndWhile
			\Return $Q$
			\EndProcedure
		\end{algorithmic}
		\caption{SARSA($\lambda$): Learn function $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$}
		\label{alg:sarsa-lambda}
	\end{algorithm}
\end{preview}

\begin{preview}
	\begin{algorithm}[H]
		\begin{algorithmic}
			\Require
			\Statex Sates $\mathcal{S} = \{1, \dots, n_s\}$
			\Statex Actions $\mathcal{A} = \{1, \dots, n_a\}$
			\Statex Reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$
			\Statex Black-box (probabilistic) transition function $T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$
			\Statex Learning rate $\alpha \in [0, 1]$, typically $\alpha = 0.1$
			\Statex Discounting factor $\gamma \in [0, 1]$
			\Procedure{Expect-SARSA}{$\mathcal{S}$, $A$, $R$, $T$, $\alpha$, $\gamma$, $\lambda$}
			\State Initialize $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ arbitrarily except set terminal states zero.
			\State Start in state $s_0 \in \mathcal{S}, s \gets s_0$
			\While{$Q$ is not converged}
			\State Select $(s_0, a) \in \mathcal{S} \times \mathcal{A}$ arbitrarily
			\While{$s$ is not terminal}
			\State $r \gets R(s, a)$ \Comment{Receive the reward}
			\State $s' \gets T(s, a)$ \Comment{Receive the new state}
			\State Calculate $\pi$ based on $Q$ (e.g. epsilon-greedy)
			\State $a' \gets \pi(s')$
            \State $Q(s, a) \gets  Q(s, a) + \alpha \cdot (r + \gamma \sum_a \pi(a | s') \cdot Q(s', a)- Q(s, a))$
			\State $s \gets s'$
			\State $a \gets a'$
			\EndWhile
			\EndWhile
			\Return $Q$
			\EndProcedure
		\end{algorithmic}
		\caption{Expect-SARSA: Learn function $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$}
		\label{alg:expect-sarsa}
	\end{algorithm}
\end{preview}

\begin{preview}
	\begin{algorithm}[H]
		\begin{algorithmic}
			\Require
			\Statex Sates $\mathcal{S} = \{1, \dots, n_s\}$
			\Statex Actions $\mathcal{A} = \{1, \dots, n_a\}$
			\Statex Reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$
			\Statex Black-box (probabilistic) transition function $T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$
			\Statex Learning rate $\alpha \in [0, 1]$, typically $\alpha = 0.1$
			\Statex Discounting factor $\gamma \in [0, 1]$
			\Procedure{SARSA}{$\mathcal{S}$, $A$, $R$, $T$, $\alpha$, $\gamma$, $\lambda$}
			\State Initialize $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ arbitrarily except set terminal states zero.
			\State Start in state $s_0 \in \mathcal{S}, s \gets s_0$
			\While{$Q$ is not converged}
			\State Select $(s_0, a) \in \mathcal{S} \times \mathcal{A}$ arbitrarily
			\While{$s$ is not terminal}
			\State $r \gets R(s, a)$ \Comment{Receive the reward}
			\State $s' \gets T(s, a)$ \Comment{Receive the new state}
			\State Calculate $\pi$ based on $Q$ (e.g. epsilon-greedy)
			\State $a' \gets \pi(s')$
            \State $Q(s, a) \gets  Q(s, a) + \alpha \cdot (r + \gamma Q(s', a')- Q(s, a))$
			\State $s \gets s'$
			\State $a \gets a'$
			\EndWhile
			\EndWhile
			\Return $Q$
			\EndProcedure
		\end{algorithmic}
		\caption{SARSA: Learn function $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$}
		\label{alg:sarsa}
	\end{algorithm}
\end{preview}

\begin{preview}
	\begin{algorithm}[H]
		\begin{algorithmic}
			\Require
			\Statex States $\mathcal{S} = \{1, \dots, n_s\}$
			\Statex Actions $\mathcal{A} = \{1, \dots, n_a\}$
			\Statex Reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$
			\Statex Black-box (probabilistic) transition function $T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$
			\Statex Learning rate $\alpha \in [0, 1]$, typically $\alpha = 0.1$
			\Statex Discounting factor $\gamma \in [0, 1]$
			\Procedure{QLearning}{$\mathcal{S}$, $A$, $R$, $T$, $\alpha$, $\gamma$}
			\State Initialize $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ arbitrarily except set terminal states zero.
			\State Start in state $s_0 \in \mathcal{S}, s \gets s_0$
			\While{$Q$ is not converged}
			\State Start in state $s \in \mathcal{S}$
			\While{$s$ is not terminal}
			\State Calculate $\pi$ based on $Q$ (e.g. epsilon-greedy)
			\State $a \gets \pi(s)$
			\State $r \gets R(s, a)$ \Comment{Receive the reward}
			\State $s' \gets T(s, a)$ \Comment{Receive the new state}
			\State $Q(s', a) \gets Q(s, a) + \alpha \cdot (r + \gamma \cdot \max_{a'} Q(s', a')-Q(s, a))$
			\State $s \gets s'$
			\EndWhile
			\EndWhile
			\Return $Q$
			\EndProcedure
		\end{algorithmic}
		\caption{$Q$-learning: Learn function $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$}
		\label{alg:q-learning}
	\end{algorithm}
\end{preview}


\end{document}